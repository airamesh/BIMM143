---
title: "Class 8 Mini-Project: Unsupervised Learning Analysis of Human Breast Cancer Cells"
format: pdf
author: Aishwarya Ramesh
---

## Exploratory data analysis

### Preparing the Data

First, getting the data:

```{r}
# Save your input data file into your Project directory
fna.data <- "WisconsinCancer.csv"

# Complete the following code to input the data and store as wisc.df
wisc.df <- read.csv(fna.data, row.names=1)
```

Next, taking a quick look at the data:

```{r}
head(wisc.df)
```

Excluding the `wisc.df$diagnosis` column in our analysis:

```{r}
# We can use -1 here to remove the first column
wisc.data <- wisc.df[,-1]
```

Checking whether it removed:

```{r}
head(wisc.data)
```

Creating `diagnosis` vector for future comparison:

```{r}
# Create diagnosis vector for later 
diagnosis <- factor(wisc.df[,1])
diagnosis
```

### Exploratory data analysis

> Q1. How many observations are in this dataset?

Finding dimensions of data:

```{r}
dim(wisc.data)
```

This dataset has 569 observations.

> Q2. How many of the observations have a malignant diagnosis?

```{r}
table(diagnosis)
```

There are 212 observations with a malignant diagnosis.

> Q3. How many variables/features in the data are suffixed with \_mean?

```{r}
wisc.columns <- colnames(wisc.data)
suffixed_list <- grep('_mean', wisc.columns)
length(suffixed_list)
```

## Principal component analysis

### Performing PCA

Checking whether data needs to be scaled:

```{r}
# Check column means and standard deviations
colMeans(wisc.data)
apply(wisc.data,2,sd)

```

There seems to be significantly different variances in the variance, so we must scale the data within PCA.

```{r}
# Perform PCA on wisc.data by completing the following code
wisc.pr <- prcomp(wisc.data, scale=TRUE, center=TRUE)
```

To get summary of principal components, we use `summary()` and we can also use `plot()` to get visual representation of proportion of variance described by each PC.

```{r}
summary(wisc.pr)
```

```{r}
plot(wisc.pr)
```

> Q4. From your results, what proportion of the original variance is captured by the first principal components (PC1)?

From the data above, we see that PC1 captures 44.27% of the original variance in the dataset.

> Q5. How many principal components (PCs) are required to describe at least 70% of the original variance in the data?

Three principal components (PC1, PC2 and PC3) are required to describe 72.64% of the variance in the original data.

> Q6. How many principal components (PCs) are required to describe at least 90% of the original variance in the data?

Seven principal components (PC1 to PC7) are required to capture 91.0% of the variance in the original data.

\### Interpreting PCA Results

First, we create a biplot of our PC data.

```{r}
biplot(wisc.pr)
```

> Q7. What stands out to you about this plot? Is it easy or difficult to understand? Why?

This plot is very hard to understand, due to the large number of dimensions in the original data and the fact that axes are labeled with rownames. They make this plot basically unreadable.

Let's try creating scatterplot of PC1 and PC2

```{r}
# Scatter plot observations by components 1 and 2
plot(x=wisc.pr$x[,1], y=wisc.pr$x[,2], col = diagnosis, 
     xlab = "PC1", ylab = "PC2")

```

The above plot has PC1 on the x-axis, PC2 on the y-axis and the points are colored by diagnosis.

> Q8. Generate a similar plot for principal components 1 and 3. What do you notice about these plots?

```{r}
# Repeat for components 1 and 3
plot(wisc.pr$x[,1], wisc.pr$x[,3], col = diagnosis, 
     xlab = "PC1", ylab = "PC3")

```

In both the above plots, the points clustered further along PC1 tend to be benign while the points less further along tend to be malignant. Moreover, benign cell points are clustered together so they tend to be more similar to each other than malignant cells are to each other. In the above figures, red color indicates malignant cells.

Using ggplot to make a fancier version of this plot:

```{r}
# Create a data.frame for ggplot
df <- as.data.frame(wisc.pr$x)
df$diagnosis <- diagnosis

# Load the ggplot2 package
library(ggplot2)

# Make a scatter plot colored by diagnosis
ggplot(df) + 
  aes(PC1, PC2, col=diagnosis) + 
  geom_point()
```

### Variance explained

Calculating variance for each principal component

```{r}
# Calculate variance of each component
pr.var <- wisc.pr$sdev^2
head(pr.var)
```

Next, calculating the proportion of variance explained by each principal component:

```{r}
# Variance explained by each principal component: pve
pve <- pr.var / sum(pr.var)

# Plot variance explained for each principal component
plot(pve, xlab = "Principal Component", 
     ylab = "Proportion of Variance Explained", 
     ylim = c(0, 1), type = "o")
```

Creating alternative plot of same data:

```{r}
# Alternative scree plot of the same data, note data driven y-axis
barplot(pve, ylab = "Precent of Variance Explained",
     names.arg=paste0("PC",1:length(pve)), las=2, axes = FALSE)
axis(2, at=pve, labels=round(pve,2)*100 )
```

Creating ggplot graph with `factoextra`

```{r}
## ggplot based graph
#install.packages("factoextra")

library(factoextra)
fviz_eig(wisc.pr, addlabels = TRUE)
```

### Communicating PCA results

> Q9. For the first principal component, what is the component of the loading vector (i.e. wisc.pr\$rotation\[,1\]) for the feature concave.points_mean?

```{r}
wisc.pr$rotation["concave.points_mean",1]
```

The PC1 value for concave.points_mean is -0.2608538.

> Q10. What is the minimum number of principal components required to explain 80% of the variance of the data?

Below we get the attributes of the summary of the data set.

```{r}
y <- summary(wisc.pr)
attributes(y)
```

Then, finding number of required components.

```{r}
num_comp = sum(y$importance[3,] <= 0.8)
num_comp
```

The number of components required to describe roughly 80% of variation in the data is 4.

## Hierarchical clustering

First, scaling wisc data.

```{r}
# Scale the wisc.data data using the "scale()" function
data.scaled <- scale(wisc.data)
```

Then, calculating Euclidean distances between all pairs of observations in scaled data.

```{r}
data.dist <- dist(data.scaled)
```

Next, creating HC model using complete linkage.

```{r}
wisc.hclust <- hclust(data.dist, method='complete')
```

> Q11. Using the plot() and abline() functions, what is the height at which the clustering model has 4 clusters?

The height at which the clustering model has 4 clusters is 19.

```{r}
plot(wisc.hclust)
abline(h=19, col="red", lty=2)
```

### Selecting number of clusters

Cutting the tree so that it has 4 clusters;

```{r}
wisc.hclust.clusters <- cutree(wisc.hclust, 4)
```

Then, comparing cluster membership to actual diagnoses:

```{r}
table(wisc.hclust.clusters, diagnosis)
```

> Q12. Can you find a better cluster vs diagnoses match by cutting into a different number of clusters between 2 and 10?

Four clusters seems to be the minimum number of clusters required for creating groups stratified by diagnosis. False positives and false negatives are not reduced by increasing the number of groups, and reducing the number of groups leads to groups that are not stratified by diagnosis. Therefore, 4 is the optimal number of clusters.

### Using different methods

> Q13. Which method gives your favorite results for the same data.dist dataset? Explain your reasoning.

Trying 'single' method

```{r}
wisc.hclust.single <- hclust(data.dist, method='single')
plot(wisc.hclust.single)

```

Trying 'average' method

```{r}
wisc.hclust.avg <- hclust(data.dist, method='average')
plot(wisc.hclust.avg)
```

Trying 'ward.D2' method

```{r}
wisc.hclust.ward <- hclust(data.dist, method='ward.D2')
plot(wisc.hclust.ward)
```

My favorite method is the ward.D2 method because it is easiest to see the clusters. The clusters are not so easily visible in the other methods.

## K-means clustering

Creating k-means model

```{r}

wisc.km <- kmeans(data.scaled, centers=2, nstart=20)

```

Using table to compare cluster membership

```{r}
table(wisc.km$cluster, diagnosis)
table(wisc.hclust.clusters, diagnosis)
```

> Q14. How well does k-means separate the two diagnoses? How does it compare to your hclust results?

K-means does a good job of separating the two diagnoses. It does a better job than the hclust method as it is able to get all the points into just two groups. 


## Combining methods

### Clustering on PCA results

First, creating distance matrix from scaled 30 PCA variables.

```{r}
scaled_pca_dist = scale(wisc.pr$x)
pca_dist = dist(wisc.pr$x)
```

Then, performing hierarchical clustering.

```{r}
wisc.pr.hclust <- hclust(pca_dist, method='ward.D2')
plot(wisc.pr.hclust)
```

Finding cluster membership vector with `cutree()` function.

```{r}
grps <- cutree(wisc.pr.hclust, k=2)
table(grps)
```

```{r}
table(grps, diagnosis)
```

Plotting the data colored by group:

```{r}
plot(wisc.pr$x[,1:2], col=grps)
```

Plotting the data colored by diagnosis:

```{r}
plot(wisc.pr$x[,1:2], col=diagnosis)
```

Recoloring:

```{r}
g <- as.factor(grps)
levels(g)
```

```{r}
g <- relevel(g,2)
levels(g)
```

```{r}
# Plot using our re-ordered factor 
plot(wisc.pr$x[,1:2], col=g)
```

Plotting with rgl

```{r}
library(rgl)
plot3d(wisc.pr$x[,1:3], xlab="PC 1", ylab="PC 2", zlab="PC 3", cex=1.5, size=1, type="s", col=grps)
```

Next, using only first 7 PC's we perform hierarchical clustering

```{r}
## Use the distance along the first 7 PCs for clustering i.e. wisc.pr$x[, 1:7]
seven_pc = dist(wisc.pr$x[ , 1:7])
wisc.pr.hclust <- hclust(seven_pc, method="ward.D2")
```

Then, cutting into 2 clusters

```{r}
wisc.pr.hclust.clusters <- cutree(wisc.pr.hclust, k=2)
```

Now, comparing results from new model with actual diagnoses

> Q15. How well does the newly created model with two clusters separate out the two diagnoses?

```{r}
# Compare to actual diagnoses
table(wisc.pr.hclust.clusters, diagnosis)
```

> Q16. How well do the k-means and hierarchical clustering models you created in previous sections (i.e. before PCA) do in terms of separating the diagnoses? Again, use the table() function to compare the output of each model (wisc.km\$cluster and wisc.hclust.clusters) with the vector containing the actual diagnoses.

Comparing to hierarchical clustering before PCA

```{r}
table(wisc.hclust.clusters, diagnosis)
```

The PCA model identifies less people who are actually malignant as benign, but identifies more people who are actually benign as malignant than the other models.

## Sensitivity/Specificity

> Q17. Which of your analysis procedures resulted in a clustering model with the best specificity? How about sensitivity?

Sensitivity for regular hclust = 0.8 Specificity for regular hclust = 0.96

Sensitivity for pca hclust = 0.886 Specificty for pca hclust = 0.922

The regular hclust model was more specific as there were less false positives. The PCA hclust model was more sensitive as it had less false negatives.

## Prediction

Getting new data and projecting into PCA space.

```{r}
#url <- "new_samples.csv"
url <- "https://tinyurl.com/new-samples-CSV"
new <- read.csv(url)
npc <- predict(wisc.pr, newdata=new)
npc
```

Plotting

```{r}
plot(wisc.pr$x[,1:2], col = g)
points(npc[,1], npc[,2], col="blue", pch=16, cex=3)
text(npc[,1], npc[,2], c(1,2), col="white")

```

> Q18. Which of these new patients should we prioritize for follow up based on your results?

We should prioritize patient 2 as their scores are in the region of the graph that has many malignant points.
